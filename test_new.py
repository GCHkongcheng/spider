#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½æ–°é—»çˆ¬è™«ç³»ç»Ÿ - æµ‹è¯•è„šæœ¬
Author: GitHub Copilot
Date: 2025-06-30
Description: æµ‹è¯•çˆ¬è™«å„ä¸ªåŠŸèƒ½æ¨¡å—
"""

import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from crawler.site_detector import SiteDetector
from crawler.universal_spider import UniversalNewsSpider
from crawler.data_manager import DataManager
from crawler.config import NEWS_SITES


def test_site_detection():
    """æµ‹è¯•ç½‘ç«™æ£€æµ‹åŠŸèƒ½"""
    print("=" * 60)
    print("           ç½‘ç«™æ£€æµ‹åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    detector = SiteDetector()
    
    # æ£€æµ‹æ‰€æœ‰ç½‘ç«™
    available_sites, results = detector.get_available_sites()
    
    print(f"\næ£€æµ‹ç»“æœ:")
    print("=" * 40)
    for site_name, result in results.items():
        status_icon = "âœ“" if result['status'] == 'success' else "âš " if result['status'] == 'partial' else "âœ—"
        print(f"{status_icon} {site_name:12}: {result['message']}")
        if result['info']:
            print(f"  â””â”€ æ ‡é¢˜: {result['info']['title'][:50]}...")
    
    print(f"\nâœ“ å¯ç”¨ç½‘ç«™æ•°é‡: {len(available_sites)}")
    
    if available_sites:
        # æ¨èæœ€ä½³ç½‘ç«™
        best_name, best_config = detector.recommend_best_site()
        print(f"âœ“ æ¨èç½‘ç«™: {best_name}")
        print(f"âœ“ ç½‘ç«™åœ°å€: {best_config['url']}")
        return best_name
    else:
        print("âœ— æ²¡æœ‰æ‰¾åˆ°å¯ç”¨ç½‘ç«™")
        return None


def test_spider_functionality(site_name):
    """æµ‹è¯•çˆ¬è™«åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("           çˆ¬è™«åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    try:
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        spider = UniversalNewsSpider(site_name=site_name)
        print(f"âœ“ çˆ¬è™«åˆå§‹åŒ–å®Œæˆ: {spider.site_name}")
        
        # çˆ¬å–å°‘é‡æ–°é—»è¿›è¡Œæµ‹è¯•
        print(f"\nå¼€å§‹æµ‹è¯•çˆ¬å– {site_name} æ–°é—»...")
        news_data = spider.crawl_news(max_count=3)
        
        if news_data:
            print(f"âœ“ æµ‹è¯•æˆåŠŸï¼Œçˆ¬å–åˆ° {len(news_data)} æ¡æ–°é—»")
            
            # æ˜¾ç¤ºæµ‹è¯•ç»“æœ
            for i, news in enumerate(news_data, 1):
                print(f"\næµ‹è¯•æ–°é—» {i}:")
                print(f"  æ ‡é¢˜: {news['title'][:50]}...")
                print(f"  æ¥æº: {news['source']}")
                print(f"  æ‘˜è¦: {news['summary'][:80]}...")
            
            return news_data
        else:
            print("âœ— æµ‹è¯•å¤±è´¥ï¼Œæœªèƒ½è·å–æ–°é—»")
            return None
            
    except Exception as e:
        print(f"âœ— çˆ¬è™«æµ‹è¯•å¤±è´¥: {e}")
        return None


def test_data_manager(news_data):
    """æµ‹è¯•æ•°æ®ç®¡ç†åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("           æ•°æ®ç®¡ç†åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    try:
        data_manager = DataManager()
        print("âœ“ æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # ç”Ÿæˆæ•°æ®ç»Ÿè®¡
        summary = data_manager.get_data_summary(news_data)
        print(f"\næ•°æ®ç»Ÿè®¡:")
        for key, value in summary.items():
            if isinstance(value, dict):
                print(f"  {key}:")
                for sub_key, sub_value in value.items():
                    print(f"    {sub_key}: {sub_value}")
            else:
                print(f"  {key}: {value}")
        
        # ä¿å­˜æµ‹è¯•æ•°æ®
        print(f"\nä¿å­˜æµ‹è¯•æ•°æ®:")
        save_results = data_manager.save_all_formats(news_data, filename_prefix="test")
        
        for format_type, filepath in save_results.items():
            if filepath:
                print(f"âœ“ {format_type.upper()}æ ¼å¼: {filepath}")
            else:
                print(f"âœ— {format_type.upper()}æ ¼å¼ä¿å­˜å¤±è´¥")
        
        return True
        
    except Exception as e:
        print(f"âœ— æ•°æ®ç®¡ç†æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ™ºèƒ½æ–°é—»çˆ¬è™«ç³»ç»Ÿæµ‹è¯•")
    
    # æµ‹è¯•1: ç½‘ç«™æ£€æµ‹
    best_site = test_site_detection()
    
    if not best_site:
        print("\nâŒ æµ‹è¯•ç»ˆæ­¢ï¼šæ²¡æœ‰å¯ç”¨çš„æ–°é—»ç½‘ç«™")
        return
    
    # æµ‹è¯•2: çˆ¬è™«åŠŸèƒ½
    news_data = test_spider_functionality(best_site)
    
    if not news_data:
        print("\nâŒ æµ‹è¯•ç»ˆæ­¢ï¼šçˆ¬è™«åŠŸèƒ½æµ‹è¯•å¤±è´¥")
        return
    
    # æµ‹è¯•3: æ•°æ®ç®¡ç†
    data_success = test_data_manager(news_data)
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("           æµ‹è¯•ç»“æœæ€»ç»“")
    print("=" * 60)
    
    if data_success:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿè¿è¡Œæ­£å¸¸")
        print("ğŸ’¡ æ‚¨ç°åœ¨å¯ä»¥è¿è¡Œå®Œæ•´çš„çˆ¬è™«ç¨‹åº:")
        print("   python run.py")
        print("   æˆ–è€…")
        print("   python crawler/main.py")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
