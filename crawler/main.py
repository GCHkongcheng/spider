#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šç”¨æ–°é—»çˆ¬è™«ä¸»ç¨‹åº
Author: GitHub Copilot
Date: 2025-06-30
Description: æ™ºèƒ½æ£€æµ‹å¹¶çˆ¬å–å¤šä¸ªæ–°é—»ç½‘ç«™
"""

import sys
import os
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from crawler.universal_spider import UniversalNewsSpider
from crawler.data_manager import DataManager
from crawler.site_detector import SiteDetector
from crawler.config import NEWS_SITES


def show_banner():
    """æ˜¾ç¤ºç¨‹åºæ¨ªå¹…"""
    print("=" * 70)
    print("                    æ™ºèƒ½æ–°é—»çˆ¬è™«ç³»ç»Ÿ")
    print("                  Universal News Crawler")
    print("=" * 70)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()


def select_news_site():
    """ç”¨æˆ·é€‰æ‹©æ–°é—»ç½‘ç«™"""
    print("ğŸ” æ­£åœ¨æ£€æµ‹å¯ç”¨çš„æ–°é—»ç½‘ç«™...")
    detector = SiteDetector()
    available_sites, detection_results = detector.get_available_sites()
    
    # æ˜¾ç¤ºæ£€æµ‹ç»“æœ
    print("\n" + "=" * 50)
    print("ç½‘ç«™æ£€æµ‹ç»“æœ:")
    print("=" * 50)
    
    for site_name, result in detection_results.items():
        if result['status'] == 'success':
            icon = "âœ…"
        elif result['status'] == 'partial':
            icon = "âš ï¸"
        else:
            icon = "âŒ"
        
        print(f"{icon} {site_name:12} - {result['message']}")
        if result['info']:
            print(f"   â””â”€ ç½‘ç«™æ ‡é¢˜: {result['info']['title'][:40]}...")
    
    if not available_sites:
        print("\nâŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ–°é—»ç½‘ç«™ï¼Œç¨‹åºæ— æ³•ç»§ç»­")
        return None
    
    # ç”¨æˆ·é€‰æ‹©
    print(f"\nâœ… å‘ç° {len(available_sites)} ä¸ªå¯ç”¨ç½‘ç«™")
    print("\nè¯·é€‰æ‹©è¦çˆ¬å–çš„æ–°é—»ç½‘ç«™:")
    print("0. è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç½‘ç«™")
    
    site_list = list(available_sites.keys())
    for i, site_name in enumerate(site_list, 1):
        print(f"{i}. {site_name}")
    
    while True:
        try:
            choice = input(f"\nè¯·è¾“å…¥é€‰æ‹© (0-{len(site_list)}): ").strip()
            choice_num = int(choice)
            
            if choice_num == 0:
                # è‡ªåŠ¨é€‰æ‹©
                best_name, _ = detector.recommend_best_site()
                print(f"âœ… è‡ªåŠ¨é€‰æ‹©: {best_name}")
                return best_name
            elif 1 <= choice_num <= len(site_list):
                selected_site = site_list[choice_num - 1]
                print(f"âœ… ç”¨æˆ·é€‰æ‹©: {selected_site}")
                return selected_site
            else:
                print("âŒ é€‰æ‹©æ— æ•ˆï¼Œè¯·é‡æ–°è¾“å…¥")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æ•°å­—")
        except KeyboardInterrupt:
            print("\n\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
            return None


def main():
    """ä¸»å‡½æ•°"""
    show_banner()
    
    try:
        # ç”¨æˆ·é€‰æ‹©æ–°é—»ç½‘ç«™
        selected_site = select_news_site()
        if not selected_site:
            return
        
        print(f"\n{'='*50}")
        print(f"å¼€å§‹çˆ¬å–: {selected_site}")
        print(f"ç½‘ç«™åœ°å€: {NEWS_SITES[selected_site]['url']}")
        print(f"{'='*50}")
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        spider = UniversalNewsSpider(site_name=selected_site)
        print("âœ… çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
        
        # åˆ›å»ºæ•°æ®ç®¡ç†å™¨
        data_manager = DataManager()
        print("âœ… æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # å¼€å§‹çˆ¬å–æ–°é—»
        print(f"\nğŸš€ å¼€å§‹çˆ¬å– {selected_site} æ–°é—»...")
        news_data = spider.crawl_news()
        
        if not news_data:
            print("âŒ æœªèƒ½è·å–åˆ°ä»»ä½•æ–°é—»æ•°æ®")
            return
        
        print(f"\nâœ… æˆåŠŸçˆ¬å– {len(news_data)} æ¡æ–°é—»")
        
        # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
        print(f"\n{'='*50}")
        print("ğŸ“Š æ•°æ®ç»Ÿè®¡æ‘˜è¦:")
        print(f"{'='*50}")
        summary = data_manager.get_data_summary(news_data)
        for key, value in summary.items():
            if isinstance(value, dict):
                print(f"{key}:")
                for sub_key, sub_value in value.items():
                    print(f"  ğŸ“Œ {sub_key}: {sub_value}")
            else:
                print(f"ğŸ“Œ {key}: {value}")
        
        # ä¿å­˜æ•°æ®
        print(f"\n{'='*50}")
        print("ğŸ’¾ ä¿å­˜æ•°æ®:")
        print(f"{'='*50}")
        save_results = data_manager.save_all_formats(news_data)
        
        for format_type, filepath in save_results.items():
            if filepath:
                print(f"âœ… {format_type.upper()}æ ¼å¼: {filepath}")
            else:
                print(f"âŒ {format_type.upper()}æ ¼å¼ä¿å­˜å¤±è´¥")
        
        # æ˜¾ç¤ºæ–°é—»é¢„è§ˆ
        print(f"\n{'='*50}")
        print("ğŸ“° æ–°é—»é¢„è§ˆ (å‰3æ¡):")
        print(f"{'='*50}")
        for i, news in enumerate(news_data[:3], 1):
            print(f"\nğŸ“° æ–°é—» {i}:")
            print(f"   ğŸ·ï¸  æ ‡é¢˜: {news['title']}")
            print(f"   ğŸ“ æ‘˜è¦: {news['summary'][:100]}{'...' if len(news['summary']) > 100 else ''}")
            print(f"   ğŸ”— é“¾æ¥: {news['url']}")
            print(f"   ğŸŒ æ¥æº: {news['source']}")
            print(f"   â° æ—¶é—´: {news['crawl_time']}")
        
        if len(news_data) > 3:
            print(f"\n... è¿˜æœ‰ {len(news_data) - 3} æ¡æ–°é—»å·²ä¿å­˜åˆ°æ–‡ä»¶ä¸­")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print(f"\nç¨‹åºç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 70)


if __name__ == "__main__":
    main()