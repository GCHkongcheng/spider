# 智能通用新闻爬虫系统

## 项目简介

这是一个智能的通用新闻爬虫系统，支持多个主流新闻网站的新闻爬取，具备自动网站检测和选择功能。系统会智能判断网站的可访问性和内容结构，自动选择最适合的新闻源进行爬取。

## 功能特点

- **多网站支持**: 支持网易财经、新浪财经、腾讯财经、央视网财经、人民网财经等多个新闻网站
- **智能检测**: 自动检测网站可用性和内容结构有效性
- **自动选择**: 智能推荐最佳可用的新闻网站
- **用户友好**: 提供交互式界面，用户可手动选择新闻源
- **多格式保存**: 支持 JSON、CSV、Excel 等多种数据保存格式
- **智能反爬**: 随机 User-Agent、请求间隔等反爬虫策略
- **完整日志**: 详细的爬取过程日志记录
- **错误处理**: 完善的错误处理和重试机制
- **数据统计**: 自动生成爬取数据的统计摘要

## 支持的新闻网站

| 网站名称   | 网址                          | 状态       |
| ---------- | ----------------------------- | ---------- |
| 网易财经   | https://money.163.com/        | [支持]     |
| 新浪财经   | https://finance.sina.com.cn/  | [支持]     |
| 腾讯财经   | https://finance.qq.com/       | [部分支持] |
| 央视网财经 | http://finance.cctv.com/      | [支持]     |
| 人民网财经 | http://finance.people.com.cn/ | [支持]     |

## 项目结构

```
crawler/
├── crawler/                     # 爬虫核心模块
│   ├── __init__.py             # 包初始化
│   ├── main.py                 # 主程序入口
│   ├── universal_spider.py     # 通用爬虫类
│   ├── site_detector.py        # 网站检测器
│   ├── data_manager.py         # 数据管理类
│   ├── config.py               # 配置文件
│   ├── utils.py                # 工具函数
│   └── spider.py               # 向后兼容的原爬虫类
├── data/                       # 数据存储目录
│   ├── spider.log              # 爬虫日志
│   ├── *.json                  # JSON格式数据
│   ├── *.csv                   # CSV格式数据
│   └── *.xlsx                  # Excel格式数据
├── .venv/                      # 虚拟环境目录
├── requirements.txt            # 依赖包列表
├── run.py                      # 快速运行脚本
├── generate_docx_report.py     # 实验报告生成脚本
├── 智能通用新闻爬虫系统实验报告.docx  # 实验报告文档
└── README.md                   # 项目说明
```

## 环境要求

- **Python**: 3.7+ (推荐 3.8+)
- **操作系统**: Windows / macOS / Linux
- **内存**: 最少 512MB 可用内存
- **网络**: 稳定的互联网连接
- **依赖包**: 见 requirements.txt 文件

### 主要依赖包

- `requests>=2.31.0` - HTTP 请求库
- `beautifulsoup4>=4.12.2` - HTML 解析库
- `lxml>=4.9.3` - XML/HTML 解析器
- `pandas>=2.0.3` - 数据处理库
- `fake-useragent>=1.4.0` - 随机 User-Agent
- `openpyxl>=3.1.0` - Excel 文件处理

## 安装步骤

1. 克隆或下载项目到本地

2. 安装依赖包：

```bash
pip install -r requirements.txt
```

## 如何运行项目

### 快速开始

**直接运行爬虫程序**：

```bash
# 方法一：使用快速运行脚本（推荐）
D:/Code/Py/crawler/.venv/Scripts/python.exe run.py

# 方法二：直接运行主程序
D:/Code/Py/crawler/.venv/Scripts/python.exe crawler/main.py
```

### 运行流程

1. **启动程序**：系统显示欢迎横幅
2. **网站检测**：自动检测所有配置的新闻网站可用性
3. **用户选择**：
   - 选择 `0` - 自动选择最佳可用网站
   - 选择 `1-N` - 手动选择特定网站
4. **开始爬取**：系统开始爬取选定网站的新闻
5. **数据保存**：自动保存为 JSON、CSV、Excel 三种格式
6. **结果展示**：显示爬取统计和新闻预览

### 程序特色

✨ **智能检测**：自动检测网站连通性和内容结构
✨ **用户友好**：提供清晰的选择界面和进度提示
✨ **多网站支持**：支持 4+ 个主流财经新闻网站
✨ **数据完整**：提供详细的统计信息和数据预览
✨ **格式多样**：同时保存 JSON、CSV、Excel 格式便于后续处理
✨ **智能过滤**：自动过滤无效标题和内容，提高数据质量

### 网站支持说明

- **完全支持**：网易财经、新浪财经、央视网财经、人民网财经
- **部分支持**：腾讯财经（网站可访问，但由于动态加载内容，选择器配置需要进一步优化）

## 运行效果

```
======================================================================
                    智能新闻爬虫系统
                  Universal News Crawler
======================================================================
开始时间: 2025-07-07 16:37:00

[INFO] 正在检测可用的新闻网站...

==================================================
网站检测结果:
==================================================
[OK] 网易财经       - 网站结构检测通过
   └─ 网站标题: 网易财经-有态度的财经门户...
[OK] 新浪财经       - 网站结构检测通过
   └─ 网站标题: 新浪财经_金融信息服务商...
[WARN] 腾讯财经     - 未找到有效的标题和链接选择器
   └─ 网站标题: 腾讯网-QQ.COM...
[OK] 央视网财经     - 网站结构检测通过
   └─ 网站标题: 财经频道_央视网...
[OK] 人民网财经     - 网站结构检测通过
   └─ 网站标题: 经济·科技--人民网...

[INFO] 发现 4 个可用网站

请选择要爬取的新闻网站:
0. 自动选择最佳网站
1. 网易财经
2. 新浪财经
3. 央视网财经
4. 人民网财经

请输入选择 (0-4): 0
[AUTO] 自动选择: 网易财经

==================================================
开始爬取: 网易财经
网站地址: https://money.163.com/
==================================================
[OK] 爬虫初始化完成
[OK] 数据管理器初始化完成

[START] 开始爬取 网易财经 新闻...
```

## 数据格式

### JSON 格式

```json
[
  {
    "title": "新闻标题",
    "url": "新闻链接",
    "summary": "新闻摘要内容，包含详细的新闻概要信息...",
    "crawl_time": "2025-07-07 16:37:10",
    "source": "网易财经"
  }
]
```

### CSV 格式

```csv
title,url,summary,crawl_time,source
新闻标题,新闻链接,新闻摘要内容，包含详细的新闻概要信息...,2025-07-07 16:37:10,网易财经
```

### Excel 格式

数据保存为 `.xlsx` 格式，包含以下列：

- **title**: 新闻标题
- **url**: 新闻链接
- **summary**: 新闻摘要
- **crawl_time**: 爬取时间
- **source**: 新闻来源

## 使用技巧

### 最佳实践

1. **首次使用**:

   ```bash
   # 直接运行爬虫程序
   python run.py
   ```

2. **批量爬取**:

   - 选择"0"自动选择最佳网站
   - 系统会智能推荐最稳定的新闻源

3. **数据分析**:

   - JSON 格式适合程序处理
   - CSV 格式适合 Excel 分析
   - Excel 格式适合直接查看

4. **定制配置**:
   - 修改 `crawler/config.py` 调整爬取参数
   - 调整 `MAX_NEWS_COUNT` 控制爬取数量
   - 调整 `DELAY_RANGE` 控制请求间隔

### 命令行选项

```bash
# 快速运行（推荐）
python run.py

# 直接运行主程序
python crawler/main.py

# 生成实验报告
python generate_docx_report.py
```

## 注意事项

1. **遵守 robots.txt**: 请在使用前检查目标网站的 robots.txt 文件
2. **适度爬取**: 避免过于频繁的请求，以免给服务器造成压力
3. **法律合规**: 仅用于学习和研究目的，不得用于商业用途
4. **网站变化**: 网站结构可能会发生变化，需要相应更新选择器

## 故障排除

### 常见问题

1. **无法获取新闻**:

   - 检查网络连接是否正常
   - 确认目标网站是否可访问
   - 查看 `data/spider.log` 日志文件获取详细错误信息
   - 尝试选择不同的新闻网站

2. **数据保存失败**:

   - 检查 `data` 目录是否存在和有写入权限
   - 确认磁盘空间充足
   - 检查文件是否被其他程序占用（如 Excel 打开了输出文件）

3. **依赖包安装失败**:

   - 升级 pip: `python -m pip install --upgrade pip`
   - 使用国内镜像: `pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple/`
   - 检查 Python 版本是否符合要求

4. **腾讯财经无法爬取**:

   - 这是已知问题，腾讯财经使用动态加载内容
   - 建议选择其他网站进行爬取
   - 或等待后续版本的修复

5. **程序运行缓慢**:
   - 这是正常现象，系统设置了请求间隔以避免被反爬
   - 可以在 `config.py` 中调整 `DELAY_RANGE` 参数

### 调试工具

- **查看日志**: 检查 `data/spider.log` 文件
- **生成报告**: `python generate_docx_report.py`
- **直接运行**: `python run.py` 进行实时调试

## 更新日志

### v1.1.0 (2025-07-07)

- **代码优化**: 修复正则表达式转义问题
- **功能增强**: 改进数据清理和过滤逻辑
- **文档更新**: 更新 README 文档，反映实际项目状态
- **问题修复**: 修复多个小问题，提高系统稳定性
- **测试完善**: 添加项目问题检查脚本

### v1.0.0 (2025-06-30)

- **初始版本发布**
- **基础功能**: 实现基本的新闻爬取功能
- **多格式支持**: 支持多种数据保存格式
- **错误处理**: 添加完整的错误处理机制
- **智能检测**: 实现网站自动检测和选择功能

## 许可证

本项目仅供学习和研究使用，请遵守相关法律法规。

## 作者

GCH 空城

## 反馈

如有问题或建议，请提交 Issue 或 Pull Request。
